name: ML Training Pipeline

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
      - '.github/workflows/main.yml'
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
  workflow_dispatch:
    inputs:
      model_type:
        description: 'Model to train (all, Logistic_Regression, Random_Forest, Gradient_Boosting, Decision_Tree, K_Nearest_Neighbors, Support_Vector_Machine)'
        required: false
        default: 'all'

permissions:
  contents: write
  actions: read

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup-python.outputs.python-version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Verify dataset
        run: |
          if [ ! -f "MLProject/Heart_Disease_preprocessing.csv" ]; then
            echo "‚ùå Dataset not found!"
            exit 1
          fi
          echo "‚úÖ Dataset found"
          python -c "import pandas as pd; df = pd.read_csv('MLProject/Heart_Disease_preprocessing.csv'); print(f'Dataset shape: {df.shape}')"

  train:
    name: Train ML Models
    needs: setup
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        model: [
          'Logistic_Regression',
          'Random_Forest',
          'Gradient_Boosting',
          'Decision_Tree',
          'K_Nearest_Neighbors',
          'Support_Vector_Machine'
        ]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Train model via MLflow Project - ${{ matrix.model }}
        run: |
          cd MLProject
          mlflow run . -P model_type=${{ matrix.model }} --env-manager=local
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model }}
          path: |
            MLProject/mlruns/
            MLProject/model_comparison_results.csv
          retention-days: 30
      
      - name: Upload training logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.model }}
          path: MLProject/*.log
          retention-days: 7
          if-no-files-found: ignore

  evaluate:
    name: Evaluate and Compare Models
    needs: train
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy matplotlib seaborn
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Compare model results
        run: |
          python -c "
          import pandas as pd
          import os
          import glob
          
          # Find all result CSV files
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if csv_files:
              # Combine results
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
              combined = combined.sort_values('test_accuracy', ascending=False)
              
              print('='*70)
              print('üìä MODEL COMPARISON RESULTS')
              print('='*70)
              print(combined.to_string(index=False))
              print('='*70)
              
              # Save combined results
              combined.to_csv('final_model_comparison.csv', index=False)
              
              # Best model
              best_model = combined.iloc[0]
              print(f\"\\nüèÜ BEST MODEL: {best_model['model']}\")
              print(f\"   Accuracy: {best_model['test_accuracy']:.4f}\")
              print(f\"   Precision: {best_model['test_precision']:.4f}\")
              print(f\"   Recall: {best_model['test_recall']:.4f}\")
              print(f\"   F1-Score: {best_model['test_f1_score']:.4f}\")
          else:
              print('‚ùå No result files found!')
          "
      
      - name: Upload final comparison
        uses: actions/upload-artifact@v4
        with:
          name: final-comparison
          path: final_model_comparison.csv
          retention-days: 90
      
      - name: Create summary
        run: |
          echo "## üéØ ML Training Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Models Trained:" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Logistic Regression" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Random Forest" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Gradient Boosting" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Decision Tree" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ K-Nearest Neighbors" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Support Vector Machine" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "final_model_comparison.csv" ]; then
            echo "### üìä Model Performance:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat final_model_comparison.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Prepare artifacts for commit
        run: |
          mkdir -p artifacts_permanent
          
          # Copy final comparison
          if [ -f "final_model_comparison.csv" ]; then
            cp final_model_comparison.csv artifacts_permanent/
          fi
          
          # Copy all downloaded artifacts
          if [ -d "artifacts" ]; then
            cp -r artifacts/* artifacts_permanent/
          fi
          
          # Create timestamp and metadata
          echo "# Training Run - $(date '+%Y-%m-%d %H:%M:%S')" > artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "**Commit:** ${{ github.sha }}" >> artifacts_permanent/README.md
          echo "**Branch:** ${{ github.ref_name }}" >> artifacts_permanent/README.md
          echo "**Workflow:** ${{ github.run_id }}" >> artifacts_permanent/README.md
          echo "**Triggered by:** ${{ github.actor }}" >> artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "## Results" >> artifacts_permanent/README.md
          if [ -f "final_model_comparison.csv" ]; then
            echo '```' >> artifacts_permanent/README.md
            cat final_model_comparison.csv >> artifacts_permanent/README.md
            echo '```' >> artifacts_permanent/README.md
          fi
      
      - name: Commit artifacts to artifacts branch
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # Checkout or create artifacts branch
          git fetch origin artifacts || true
          git checkout -B artifacts || git checkout --orphan artifacts
          
          # Clear existing content (keep only new artifacts)
          git rm -rf . || true
           
          # Copy new artifacts
          cp -r artifacts_permanent/* . 2>/dev/null || echo "No artifacts to copy"
          
          # Commit and push with token
          git add . || true
          git commit -m "ü§ñ Auto-commit: Training artifacts from run ${{ github.run_id }}" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git artifacts --force || echo "Push failed"

  docker:
    name: Build and Push Docker Image
    needs: evaluate
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Verify Docker Hub credentials
        run: |
          if [ -z "${{ secrets.DOCKERHUB_USERNAME }}" ]; then
            echo "‚ùå DOCKERHUB_USERNAME secret is not set!"
            echo "Please add it in: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo ""
            echo "To skip Docker build, this is normal - training will still complete successfully."
            exit 1
          fi
          
          if [ -z "${{ secrets.DOCKER_PASSWORD }}" ]; then
            echo "‚ùå DOCKER_PASSWORD secret is not set!"
            echo "Please add it in: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo ""
            echo "To skip Docker build, this is normal - training will still complete successfully."
            exit 1
          fi
          
          echo "‚úÖ Docker Hub credentials are configured"
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install MLflow
        run: |
          python -m pip install --upgrade pip
          pip install mlflow==2.19.0 boto3
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Find best model
        id: best-model
        run: |
          python -c "
          import pandas as pd
          import glob
          import os
          
          # Search in all artifact folders
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if not csv_files:
              print('‚ö†Ô∏è  No CSV files found with glob pattern')
              print('üìÇ Checking artifacts directory structure...')
              import os
              for root, dirs, files in os.walk('artifacts'):
                  level = root.replace('artifacts', '').count(os.sep)
                  indent = ' ' * 2 * level
                  print(f'{indent}{os.path.basename(root)}/')
                  subindent = ' ' * 2 * (level + 1)
                  for file in files:
                      print(f'{subindent}{file}')
              
              print('\\nüîç Searching for any CSV files...')
              all_csv = glob.glob('artifacts/**/*.csv', recursive=True)
              print(f'Found {len(all_csv)} CSV files:')
              for f in all_csv:
                  print(f'  - {f}')
              
              # Try alternative paths
              csv_files = glob.glob('artifacts/*/MLProject/model_comparison_results.csv')
              if not csv_files:
                  csv_files = glob.glob('artifacts/*/model_comparison_results.csv')
              
              if not csv_files:
                  print('\\n‚ùå Still no results found!')
                  exit(1)
          
          print(f'\\n‚úÖ Found {len(csv_files)} result files')
          for f in csv_files:
              print(f'  - {f}')
          
          # Read and combine results
          dfs = [pd.read_csv(f) for f in csv_files]
          combined = pd.concat(dfs, ignore_index=True)
          combined = combined.sort_values('test_accuracy', ascending=False)
          
          print('\\nüìä Combined Results:')
          print(combined.to_string(index=False))
          
          best_model = combined.iloc[0]['model']
          best_accuracy = combined.iloc[0]['test_accuracy']
          
          # Sanitize model name for Docker tag
          model_tag = best_model.lower().replace('_', '-')
          
          print(f'\\nüèÜ Best model: {best_model}')
          print(f'   Accuracy: {best_accuracy:.4f}')
          print(f'   Docker tag: {model_tag}')
          
          # Write to GitHub output
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'model_name={best_model}\\n')
              f.write(f'model_tag={model_tag}\\n')
              f.write(f'accuracy={best_accuracy}\\n')
          "
      
      - name: Prepare model for Docker
        run: |
          MODEL_NAME="${{ steps.best-model.outputs.model_name }}"
          echo "üîß Preparing model: $MODEL_NAME"
          
          # Create directory for model
          mkdir -p model_to_deploy
          
          echo "üì¶ Extracting model artifacts..."
          # Find model artifacts from the specific model
          MODEL_ARTIFACT_DIR=$(find artifacts/model-${MODEL_NAME} -type d -path "*/artifacts/model" | head -1)
          
          if [ -z "$MODEL_ARTIFACT_DIR" ]; then
            echo "‚ùå Model artifacts not found for $MODEL_NAME"
            echo "Searching in all artifacts..."
            find artifacts -name "MLmodel" -type f
            exit 1
          fi
          
          echo "‚úÖ Found model at: $MODEL_ARTIFACT_DIR"
          
          # Copy model artifacts
          cp -r "$MODEL_ARTIFACT_DIR" model_to_deploy/
          
          echo ""
          echo "üìÇ Model structure:"
          ls -la model_to_deploy/model/
          
          echo ""
          echo "üìÑ MLmodel file:"
          cat model_to_deploy/model/MLmodel
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build Docker image with MLflow
        env:
          MODEL_NAME: ${{ steps.best-model.outputs.model_name }}
          MODEL_TAG: ${{ steps.best-model.outputs.model_tag }}
          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
        run: |
          echo "üê≥ Building Docker image for: $MODEL_NAME"
          
          # Build Docker image using MLflow with local model path
          IMAGE_NAME="${DOCKER_USERNAME}/heart-disease-model"
          IMAGE_TAG="${MODEL_TAG}-$(date +%Y%m%d)"
          
          echo ""
          echo "üì¶ Image details:"
          echo "   Name: ${IMAGE_NAME}"
          echo "   Tag: ${IMAGE_TAG}"
          echo "   Model path: $(pwd)/model_to_deploy/model"
          
          # Build using local model path (not runs URI)
          echo ""
          echo "üî® Building Docker image..."
          mlflow models build-docker \
            --model-uri "$(pwd)/model_to_deploy/model" \
            --name "${IMAGE_NAME}"
          
          echo "‚úÖ Docker image built successfully"
          
          # Tag with version and latest
          echo ""
          echo "üè∑Ô∏è  Tagging images..."
          docker tag "${IMAGE_NAME}:latest" "${IMAGE_NAME}:${IMAGE_TAG}"
          docker tag "${IMAGE_NAME}:latest" "${IMAGE_NAME}:${MODEL_TAG}"
          
          echo "‚úÖ Tags created:"
          echo "   - ${IMAGE_NAME}:latest"
          echo "   - ${IMAGE_NAME}:${IMAGE_TAG}"
          echo "   - ${IMAGE_NAME}:${MODEL_TAG}"
          
          # Save to env for next step
          echo "IMAGE_NAME=${IMAGE_NAME}" >> $GITHUB_ENV
          echo "IMAGE_TAG=${IMAGE_TAG}" >> $GITHUB_ENV
      
      - name: Push Docker image to Docker Hub
        env:
          IMAGE_NAME: ${{ env.IMAGE_NAME }}
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          MODEL_TAG: ${{ steps.best-model.outputs.model_tag }}
        run: |
          echo "Pushing Docker images to Docker Hub..."
          
          # Push all tags
          docker push ${IMAGE_NAME}:latest
          docker push ${IMAGE_NAME}:${IMAGE_TAG}
          docker push ${IMAGE_NAME}:${MODEL_TAG}
          
          echo "‚úÖ Docker images pushed successfully!"
          echo "   - ${IMAGE_NAME}:latest"
          echo "   - ${IMAGE_NAME}:${IMAGE_TAG}"
          echo "   - ${IMAGE_NAME}:${MODEL_TAG}"
      
      - name: Create Docker summary
        env:
          IMAGE_NAME: ${{ env.IMAGE_NAME }}
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          MODEL_NAME: ${{ steps.best-model.outputs.model_name }}
          ACCURACY: ${{ steps.best-model.outputs.accuracy }}
        run: |
          echo "## üê≥ Docker Image Published" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Model:** ${MODEL_NAME}" >> $GITHUB_STEP_SUMMARY
          echo "**Accuracy:** ${ACCURACY}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Docker Images:" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "docker pull ${IMAGE_NAME}:latest" >> $GITHUB_STEP_SUMMARY
          echo "docker pull ${IMAGE_NAME}:${IMAGE_TAG}" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Run Container:" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo "docker run -p 8080:8080 ${IMAGE_NAME}:latest" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

  notify:
    name: Notification
    needs: [train, evaluate, docker]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Training Success Notification
        if: needs.train.result == 'success' && needs.evaluate.result == 'success'
        run: |
          echo "‚úÖ All models trained successfully!"
          echo "üìä Evaluation completed!"
      
      - name: Docker Success Notification
        if: needs.docker.result == 'success'
        run: |
          echo "üê≥ Docker image pushed to Docker Hub!"
      
      - name: Docker Skipped Notification
        if: needs.docker.result == 'skipped'
        run: |
          echo "‚ö†Ô∏è Docker build was skipped"
          echo ""
          echo "Possible reasons:"
          echo "  1. DOCKER_USERNAME secret not set"
          echo "  2. DOCKER_PASSWORD secret not set"
          echo ""
          echo "To enable Docker deployment:"
          echo "  1. Go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
          echo "  2. Add secret: DOCKER_USERNAME (your Docker Hub username)"
          echo "  3. Add secret: DOCKER_PASSWORD (your Docker Hub access token)"
          echo ""
          echo "üìñ See DOCKER_SETUP.md for detailed instructions"
      
      - name: Training Failure Notification
        if: needs.train.result == 'failure' || needs.evaluate.result == 'failure'
        run: |
          echo "‚ùå Training or evaluation failed!"
          echo "Please check the logs for details."
          exit 1
      
      - name: Docker Failure Notification
        if: needs.docker.result == 'failure'
        run: |
          echo "‚ö†Ô∏è Docker build/push failed!"
          echo "Training was successful but Docker image not created."
          echo ""
          echo "Common issues:"
          echo "  1. Check DOCKER_USERNAME and DOCKER_PASSWORD secrets"
          echo "  2. Verify Docker Hub access token has correct permissions"
          echo "  3. Check Docker Hub storage quota"
          echo "  4. Review workflow logs for specific error"
