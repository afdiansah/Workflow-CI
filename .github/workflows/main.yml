name: ML Training Pipeline

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
      - '.github/workflows/main.yml'
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
  workflow_dispatch:
    inputs:
      model_type:
        description: 'Model to train (all, Logistic_Regression, Random_Forest, Gradient_Boosting, Decision_Tree, K_Nearest_Neighbors, Support_Vector_Machine)'
        required: false
        default: 'all'

permissions:
  contents: write
  actions: read

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup-python.outputs.python-version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Verify dataset
        run: |
          if [ ! -f "MLProject/Heart_Disease_preprocessing.csv" ]; then
            echo "âŒ Dataset not found!"
            exit 1
          fi
          echo "âœ… Dataset found"
          python -c "import pandas as pd; df = pd.read_csv('MLProject/Heart_Disease_preprocessing.csv'); print(f'Dataset shape: {df.shape}')"

  train:
    name: Train ML Models
    needs: setup
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        model: [
          'Logistic_Regression',
          'Random_Forest',
          'Gradient_Boosting',
          'Decision_Tree',
          'K_Nearest_Neighbors',
          'Support_Vector_Machine'
        ]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Train model - ${{ matrix.model }}
        run: |
          cd MLProject
          python modelling.py --model ${{ matrix.model }}
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model }}
          path: |
            MLProject/mlruns/
            MLProject/model_comparison_results.csv
          retention-days: 30
      
      - name: Upload training logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.model }}
          path: MLProject/*.log
          retention-days: 7
          if-no-files-found: ignore

  evaluate:
    name: Evaluate and Compare Models
    needs: train
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy matplotlib seaborn
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Compare model results
        run: |
          python -c "
          import pandas as pd
          import os
          import glob
          
          # Find all result CSV files
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if csv_files:
              # Combine results
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
              combined = combined.sort_values('test_accuracy', ascending=False)
              
              print('='*70)
              print('ğŸ“Š MODEL COMPARISON RESULTS')
              print('='*70)
              print(combined.to_string(index=False))
              print('='*70)
              
              # Save combined results
              combined.to_csv('final_model_comparison.csv', index=False)
              
              # Best model
              best_model = combined.iloc[0]
              print(f\"\\nğŸ† BEST MODEL: {best_model['model']}\")
              print(f\"   Accuracy: {best_model['test_accuracy']:.4f}\")
              print(f\"   Precision: {best_model['test_precision']:.4f}\")
              print(f\"   Recall: {best_model['test_recall']:.4f}\")
              print(f\"   F1-Score: {best_model['test_f1_score']:.4f}\")
          else:
              print('âŒ No result files found!')
          "
      
      - name: Upload final comparison
        uses: actions/upload-artifact@v4
        with:
          name: final-comparison
          path: final_model_comparison.csv
          retention-days: 90
      
      - name: Create summary
        run: |
          echo "## ğŸ¯ ML Training Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Models Trained:" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Logistic Regression" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Random Forest" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Gradient Boosting" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Decision Tree" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… K-Nearest Neighbors" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Support Vector Machine" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "final_model_comparison.csv" ]; then
            echo "### ğŸ“Š Model Performance:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat final_model_comparison.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Prepare artifacts for commit
        run: |
          mkdir -p artifacts_permanent
          
          # Copy final comparison
          if [ -f "final_model_comparison.csv" ]; then
            cp final_model_comparison.csv artifacts_permanent/
          fi
          
          # Copy all downloaded artifacts
          if [ -d "artifacts" ]; then
            cp -r artifacts/* artifacts_permanent/
          fi
          
          # Create timestamp and metadata
          echo "# Training Run - $(date '+%Y-%m-%d %H:%M:%S')" > artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "**Commit:** ${{ github.sha }}" >> artifacts_permanent/README.md
          echo "**Branch:** ${{ github.ref_name }}" >> artifacts_permanent/README.md
          echo "**Workflow:** ${{ github.run_id }}" >> artifacts_permanent/README.md
          echo "**Triggered by:** ${{ github.actor }}" >> artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "## Results" >> artifacts_permanent/README.md
          if [ -f "final_model_comparison.csv" ]; then
            echo '```' >> artifacts_permanent/README.md
            cat final_model_comparison.csv >> artifacts_permanent/README.md
            echo '```' >> artifacts_permanent/README.md
          fi
      
      - name: Commit artifacts to artifacts branch
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # Checkout or create artifacts branch
          git fetch origin artifacts || true
          git checkout -B artifacts || git checkout --orphan artifacts
          
          # Clear existing content (keep only new artifacts)
          git rm -rf . || true
          
          # Copy new artifacts
          cp -r artifacts_permanent/* . 2>/dev/null || echo "No artifacts to copy"
          
          # Commit and push with token
          git add . || true
          git commit -m "ğŸ¤– Auto-commit: Training artifacts from run ${{ github.run_id }}" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git artifacts --force || echo "Push failed"

  docker:
    name: Build and Push Docker Image
    needs: [train, evaluate]
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Download all model artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Find best model
        id: best_model
        run: |
          python -c "
          import pandas as pd
          import glob
          import os
          
          # Find all result CSV files
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if csv_files:
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
              combined = combined.sort_values('test_accuracy', ascending=False)
              best_model = combined.iloc[0]['model']
              print(f'Best model: {best_model}')
              
              # Save to env
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f'model_name={best_model}\n')
          else:
              print('No models found!')
              exit(1)
          "
      
      - name: Prepare model for Docker
        run: |
          MODEL_NAME="${{ steps.best_model.outputs.model_name }}"
          echo "ğŸ³ Preparing Docker image for: $MODEL_NAME"
          
          # Find MLflow model path
          MODEL_PATH=$(find artifacts/model-${MODEL_NAME}/mlruns -name "MLmodel" | head -n 1)
          if [ -z "$MODEL_PATH" ]; then
            echo "âŒ Model not found!"
            exit 1
          fi
          
          MODEL_DIR=$(dirname "$MODEL_PATH")
          echo "ğŸ“¦ Model directory: $MODEL_DIR"
          echo "MODEL_DIR=$MODEL_DIR" >> $GITHUB_ENV
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Build Docker image with MLflow
        run: |
          MODEL_NAME="${{ steps.best_model.outputs.model_name }}"
          IMAGE_NAME="${{ secrets.DOCKERHUB_USERNAME }}/heart-disease-${MODEL_NAME,,}"
          TAG="${{ github.run_id }}"
          
          echo "ğŸ”¨ Building Docker image: $IMAGE_NAME:$TAG"
          
          # Build Docker image using MLflow
          mlflow models build-docker \
            --model-uri "$MODEL_DIR" \
            --name "$IMAGE_NAME:$TAG" \
            --enable-mlserver
          
          # Tag as latest
          docker tag "$IMAGE_NAME:$TAG" "$IMAGE_NAME:latest"
          
          echo "IMAGE_NAME=$IMAGE_NAME" >> $GITHUB_ENV
          echo "TAG=$TAG" >> $GITHUB_ENV
      
      - name: Push Docker image to Docker Hub
        run: |
          IMAGE_NAME="${{ env.IMAGE_NAME }}"
          TAG="${{ env.TAG }}"
          
          echo "ğŸš€ Pushing Docker image: $IMAGE_NAME:$TAG"
          docker push "$IMAGE_NAME:$TAG"
          docker push "$IMAGE_NAME:latest"
          
          echo "âœ… Docker image pushed successfully!"
          echo "ğŸ“¦ Image: $IMAGE_NAME:$TAG"
          echo "ğŸ“¦ Image: $IMAGE_NAME:latest"
      
      - name: Create Docker deployment guide
        run: |
          MODEL_NAME="${{ steps.best_model.outputs.model_name }}"
          IMAGE_NAME="${{ env.IMAGE_NAME }}"
          TAG="${{ env.TAG }}"
          
          cat > docker-deployment-guide.md << EOF
          # ğŸ³ Docker Deployment Guide
          
          ## Model Information
          - **Model:** $MODEL_NAME
          - **Accuracy:** Best performing model from run ${{ github.run_id }}
          - **Image:** \`$IMAGE_NAME:$TAG\`
          - **Latest:** \`$IMAGE_NAME:latest\`
          
          ## Pull Image
          \`\`\`bash
          docker pull $IMAGE_NAME:$TAG
          # Or latest
          docker pull $IMAGE_NAME:latest
          \`\`\`
          
          ## Run Container
          \`\`\`bash
          docker run -p 8080:8080 $IMAGE_NAME:$TAG
          \`\`\`
          
          ## Test Prediction
          \`\`\`bash
          curl -X POST http://localhost:8080/invocations \
            -H "Content-Type: application/json" \
            -d '{
              "dataframe_split": {
                "columns": ["Age", "Sex", "Chest pain type", "BP", "Cholesterol", "FBS over 120", "EKG results", "Max HR", "Exercise angina", "ST depression", "Slope of ST", "Number of vessels fluro", "Thallium"],
                "data": [[63, 1, 1, 145, 233, 1, 2, 150, 0, 2.3, 3, 0, 6]]
              }
            }
          \`\`\`
          
          ## Docker Hub
          View on Docker Hub: https://hub.docker.com/r/${{ secrets.DOCKERHUB_USERNAME }}/heart-disease-${MODEL_NAME,,}
          EOF
          
          cat docker-deployment-guide.md
      
      - name: Upload deployment guide
        uses: actions/upload-artifact@v4
        with:
          name: docker-deployment-guide
          path: docker-deployment-guide.md
          retention-days: 90
      
      - name: Add Docker info to summary
        run: |
          MODEL_NAME="${{ steps.best_model.outputs.model_name }}"
          IMAGE_NAME="${{ env.IMAGE_NAME }}"
          TAG="${{ env.TAG }}"
          
          echo "## ğŸ³ Docker Image Built & Pushed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Model:** $MODEL_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Image:** \`$IMAGE_NAME:$TAG\`" >> $GITHUB_STEP_SUMMARY
          echo "**Latest:** \`$IMAGE_NAME:latest\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“¥ Pull & Run:" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo "docker pull $IMAGE_NAME:$TAG" >> $GITHUB_STEP_SUMMARY
          echo "docker run -p 8080:8080 $IMAGE_NAME:$TAG" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ”— **Docker Hub:** https://hub.docker.com/r/${{ secrets.DOCKERHUB_USERNAME }}/heart-disease-${MODEL_NAME,,}" >> $GITHUB_STEP_SUMMARY

  notify:
    name: Notification
    needs: [train, evaluate, docker]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Training Success Notification
        if: needs.train.result == 'success' && needs.evaluate.result == 'success'
        run: |
          echo "âœ… All models trained successfully!"
          echo "ğŸ“Š Evaluation completed!"
          
          if [ "${{ needs.docker.result }}" == "success" ]; then
            echo "ğŸ³ Docker image built and pushed!"
          fi
      
      - name: Training Failure Notification
        if: needs.train.result == 'failure' || needs.evaluate.result == 'failure'
        run: |
          echo "âŒ Training or evaluation failed!"
          echo "Please check the logs for details."
          exit 1
