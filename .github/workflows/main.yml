name: ML Training Pipeline

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
      - '.github/workflows/main.yml'
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'MLProject/**'
  workflow_dispatch:
    inputs:
      model_type:
        description: 'Model to train (all, Logistic_Regression, Random_Forest, Gradient_Boosting, Decision_Tree, K_Nearest_Neighbors, Support_Vector_Machine)'
        required: false
        default: 'all'

permissions:
  contents: write
  actions: read

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup-python.outputs.python-version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Verify dataset
        run: |
          if [ ! -f "MLProject/Heart_Disease_preprocessing.csv" ]; then
            echo "‚ùå Dataset not found!"
            exit 1
          fi
          echo "‚úÖ Dataset found"
          python -c "import pandas as pd; df = pd.read_csv('MLProject/Heart_Disease_preprocessing.csv'); print(f'Dataset shape: {df.shape}')"

  train:
    name: Train ML Models
    needs: setup
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        model: [
          'Logistic_Regression',
          'Random_Forest',
          'Gradient_Boosting',
          'Decision_Tree',
          'K_Nearest_Neighbors',
          'Support_Vector_Machine'
        ]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt
      
      - name: Train model via MLflow Project - ${{ matrix.model }}
        run: |
          cd MLProject
          mlflow run . -P model_type=${{ matrix.model }} --env-manager=local
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model }}
          path: |
            MLProject/mlruns/
            MLProject/model_comparison_results.csv
            MLProject/model_comparison_results.json
          retention-days: 30
          if-no-files-found: warn
      
      - name: Upload training logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.model }}
          path: MLProject/*.log
          retention-days: 7
          if-no-files-found: ignore

  evaluate:
    name: Evaluate and Compare Models
    needs: train
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy matplotlib seaborn
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Compare model results
        run: |
          python -c "
          import pandas as pd
          import os
          import glob
          
          # Find all result CSV files
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if csv_files:
              # Combine results
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
              combined = combined.sort_values('test_accuracy', ascending=False)
              
              print('='*70)
              print('üìä MODEL COMPARISON RESULTS')
              print('='*70)
              print(combined.to_string(index=False))
              print('='*70)
              
              # Save combined results
              combined.to_csv('final_model_comparison.csv', index=False)
              
              # Best model
              best_model = combined.iloc[0]
              print(f\"\\nüèÜ BEST MODEL: {best_model['model']}\")
              print(f\"   Accuracy: {best_model['test_accuracy']:.4f}\")
              print(f\"   Precision: {best_model['test_precision']:.4f}\")
              print(f\"   Recall: {best_model['test_recall']:.4f}\")
              print(f\"   F1-Score: {best_model['test_f1_score']:.4f}\")
          else:
              print('‚ùå No result files found!')
          "
      
      - name: Upload final comparison
        uses: actions/upload-artifact@v4
        with:
          name: final-comparison
          path: final_model_comparison.csv
          retention-days: 90
      
      - name: Create summary
        run: |
          echo "## üéØ ML Training Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Models Trained:" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Logistic Regression" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Random Forest" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Gradient Boosting" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Decision Tree" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ K-Nearest Neighbors" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Support Vector Machine" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "final_model_comparison.csv" ]; then
            echo "### üìä Model Performance:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat final_model_comparison.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Prepare artifacts for commit
        run: |
          mkdir -p artifacts_permanent
          
          # Copy final comparison
          if [ -f "final_model_comparison.csv" ]; then
            cp final_model_comparison.csv artifacts_permanent/
          fi
          
          # Copy all downloaded artifacts
          if [ -d "artifacts" ]; then
            cp -r artifacts/* artifacts_permanent/
          fi
          
          # Create timestamp and metadata
          echo "# Training Run - $(date '+%Y-%m-%d %H:%M:%S')" > artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "**Commit:** ${{ github.sha }}" >> artifacts_permanent/README.md
          echo "**Branch:** ${{ github.ref_name }}" >> artifacts_permanent/README.md
          echo "**Workflow:** ${{ github.run_id }}" >> artifacts_permanent/README.md
          echo "**Triggered by:** ${{ github.actor }}" >> artifacts_permanent/README.md
          echo "" >> artifacts_permanent/README.md
          echo "## Results" >> artifacts_permanent/README.md
          if [ -f "final_model_comparison.csv" ]; then
            echo '```' >> artifacts_permanent/README.md
            cat final_model_comparison.csv >> artifacts_permanent/README.md
            echo '```' >> artifacts_permanent/README.md
          fi
      
      - name: Commit artifacts to artifacts branch
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # Checkout or create artifacts branch
          git fetch origin artifacts || true
          git checkout -B artifacts || git checkout --orphan artifacts
          
          # Clear existing content (keep only new artifacts)
          git rm -rf . || true
           
          # Copy new artifacts
          cp -r artifacts_permanent/* . 2>/dev/null || echo "No artifacts to copy"
          
          # Commit and push with token
          git add . || true
          git commit -m "ü§ñ Auto-commit: Training artifacts from run ${{ github.run_id }}" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git artifacts --force || echo "Push failed"

  docker:
    name: Build and Push Docker Image
    needs: evaluate
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Verify Docker Hub credentials
        run: |
          if [ -z "${{ secrets.DOCKERHUB_USERNAME }}" ]; then
            echo "‚ùå DOCKERHUB_USERNAME secret is not set!"
            echo "Please add it in: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo ""
            echo "To skip Docker build, this is normal - training will still complete successfully."
            exit 1
          fi
          
          if [ -z "${{ secrets.DOCKER_PASSWORD }}" ]; then
            echo "‚ùå DOCKER_PASSWORD secret is not set!"
            echo "Please add it in: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo ""
            echo "To skip Docker build, this is normal - training will still complete successfully."
            exit 1
          fi
          
          echo "‚úÖ Docker Hub credentials are configured"
      
      - name: Set up Python 3.12.7
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.7'
      
      - name: Install MLflow
        run: |
          python -m pip install --upgrade pip
          pip install mlflow==2.19.0 boto3 pandas pyyaml
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Find best model
        id: best-model
        run: |
          python -c "
          import pandas as pd
          import glob
          import os
          import json
          
          print('üîç Searching for model results...')
          
          # Try to find CSV files first
          csv_files = glob.glob('artifacts/**/model_comparison_results.csv', recursive=True)
          
          if csv_files:
              print(f'‚úÖ Found {len(csv_files)} CSV result files')
              for f in csv_files:
                  print(f'  - {f}')
              
              # Read and combine results
              dfs = [pd.read_csv(f) for f in csv_files]
              combined = pd.concat(dfs, ignore_index=True)
          else:
              print('‚ö†Ô∏è  No CSV files found, trying JSON...')
              # Try JSON files as fallback
              json_files = glob.glob('artifacts/**/model_comparison_results.json', recursive=True)
              
              if json_files:
                  print(f'‚úÖ Found {len(json_files)} JSON result files')
                  for f in json_files:
                      print(f'  - {f}')
                  
                  # Read and combine JSON results
                  all_data = []
                  for f in json_files:
                      with open(f, 'r') as jf:
                          data = json.load(jf)
                          if isinstance(data, list):
                              all_data.extend(data)
                          else:
                              all_data.append(data)
                  combined = pd.DataFrame(all_data)
              else:
                  print('‚ö†Ô∏è  No JSON files found either, trying to read from MLflow runs...')
                  
                  # Last resort: read metrics directly from mlruns
                  import yaml
                  
                  all_results = []
                  
                  # Correct pattern for mlruns structure: artifacts/model-XXX/mlruns/0/run-id/
                  run_dirs = []
                  for model_dir in glob.glob('artifacts/model-*'):
                      mlruns_exp_dir = os.path.join(model_dir, 'mlruns', '0')
                      if os.path.exists(mlruns_exp_dir):
                          for run_id in os.listdir(mlruns_exp_dir):
                              run_path = os.path.join(mlruns_exp_dir, run_id)
                              if os.path.isdir(run_path) and run_id != 'meta.yaml':
                                  run_dirs.append(run_path)
                  
                  print(f'üìÇ Found {len(run_dirs)} MLflow run directories')
                  
                  if not run_dirs:
                      print('\\n‚ùå No run directories found!')
                      print('\\nSearching in all mlruns directories...')
                      all_mlruns = glob.glob('artifacts/*/mlruns/**/', recursive=True)
                      print(f'All mlruns paths found ({len(all_mlruns)}):')
                      for p in all_mlruns[:20]:  # Show first 20
                          print(f'  - {p}')
                  
                  for run_dir in run_dirs:
                      meta_file = os.path.join(run_dir, 'meta.yaml')
                      if os.path.exists(meta_file):
                          try:
                              with open(meta_file, 'r') as f:
                                  meta = yaml.safe_load(f)
                              
                              # Get model name from run name
                              model_name = meta.get('run_name', 'Unknown')
                              print(f'  üìä Processing run: {model_name}')
                              
                              # Read metrics
                              metrics_dir = os.path.join(run_dir, 'metrics')
                              if os.path.exists(metrics_dir):
                                  metrics = {}
                                  for metric_file in os.listdir(metrics_dir):
                                      metric_path = os.path.join(metrics_dir, metric_file)
                                      if os.path.isfile(metric_path):
                                          with open(metric_path, 'r') as f:
                                              # MLflow metrics format: timestamp value step
                                              lines = f.readlines()
                                              if lines:
                                                  last_line = lines[-1].strip()
                                                  parts = last_line.split()
                                                  if len(parts) >= 2:
                                                      value = float(parts[1])
                                                      metrics[metric_file] = value
                                  
                                  if metrics:
                                      all_results.append({
                                          'model': model_name,
                                          'test_accuracy': metrics.get('test_accuracy', 0),
                                          'test_precision': metrics.get('test_precision', 0),
                                          'test_recall': metrics.get('test_recall', 0),
                                          'test_f1_score': metrics.get('test_f1_score', 0)
                                      })
                                      print(f'    ‚úÖ Metrics extracted: accuracy={metrics.get(\"test_accuracy\", 0):.4f}')
                                  else:
                                      print(f'    ‚ö†Ô∏è  No metrics found in {metrics_dir}')
                              else:
                                  print(f'    ‚ö†Ô∏è  No metrics directory in {run_dir}')
                          except Exception as e:
                              print(f'    ‚ùå Error processing {run_dir}: {e}')
                  
                  if all_results:
                      combined = pd.DataFrame(all_results)
                      print(f'\\n‚úÖ Extracted metrics from {len(all_results)} MLflow runs')
                  else:
                      print('\\n‚ùå No results found from any source!')
                      exit(1)
          
          # Sort by accuracy
          combined = combined.sort_values('test_accuracy', ascending=False)
          
          print('\\nüìä Combined Results:')
          print(combined.to_string(index=False))
          
          best_model = combined.iloc[0]['model']
          best_accuracy = combined.iloc[0]['test_accuracy']
          
          # Sanitize model name for Docker tag
          model_tag = best_model.lower().replace('_', '-')
          
          print(f'\\nüèÜ Best model: {best_model}')
          print(f'   Accuracy: {best_accuracy:.4f}')
          print(f'   Docker tag: {model_tag}')
          
          # Write to GitHub output
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'model_name={best_model}\\n')
              f.write(f'model_tag={model_tag}\\n')
              f.write(f'accuracy={best_accuracy}\\n')
          "
      
      - name: Prepare model for Docker
        run: |
          MODEL_NAME="${{ steps.best-model.outputs.model_name }}"
          echo "üîß Preparing model: $MODEL_NAME"
          
          # Create directory for model
          mkdir -p model_to_deploy
          
          echo "üì¶ Extracting model artifacts..."
          # Find model artifacts from the specific model
          MODEL_ARTIFACT_DIR=$(find artifacts/model-${MODEL_NAME} -type d -path "*/artifacts/model" | head -1)
          
          if [ -z "$MODEL_ARTIFACT_DIR" ]; then
            echo "‚ùå Model artifacts not found for $MODEL_NAME"
            echo "Searching in all artifacts..."
            find artifacts -name "MLmodel" -type f
            exit 1
          fi
          
          echo "‚úÖ Found model at: $MODEL_ARTIFACT_DIR"
          
          # Copy model artifacts
          cp -r "$MODEL_ARTIFACT_DIR" model_to_deploy/
          
          echo ""
          echo "üìÇ Model structure:"
          ls -la model_to_deploy/model/
          
          echo ""
          echo "üìÑ MLmodel file:"
          cat model_to_deploy/model/MLmodel
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build Docker image with MLflow
        env:
          MODEL_NAME: ${{ steps.best-model.outputs.model_name }}
          MODEL_TAG: ${{ steps.best-model.outputs.model_tag }}
          DOCKER_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
        run: |
          echo "üê≥ Building Docker image for: $MODEL_NAME"
          
          # Build Docker image using MLflow with local model path
          IMAGE_NAME="${DOCKER_USERNAME}/heart-disease-model"
          IMAGE_TAG="${MODEL_TAG}-$(date +%Y%m%d)"
          
          echo ""
          echo "üì¶ Image details:"
          echo "   Name: ${IMAGE_NAME}"
          echo "   Tag: ${IMAGE_TAG}"
          echo "   Model path: $(pwd)/model_to_deploy/model"
          
          # Build using local model path (not runs URI)
          echo ""
          echo "üî® Building Docker image..."
          mlflow models build-docker \
            --model-uri "$(pwd)/model_to_deploy/model" \
            --name "${IMAGE_NAME}"
          
          echo "‚úÖ Docker image built successfully"
          
          # Tag with version and latest
          echo ""
          echo "üè∑Ô∏è  Tagging images..."
          docker tag "${IMAGE_NAME}:latest" "${IMAGE_NAME}:${IMAGE_TAG}"
          docker tag "${IMAGE_NAME}:latest" "${IMAGE_NAME}:${MODEL_TAG}"
          
          echo "‚úÖ Tags created:"
          echo "   - ${IMAGE_NAME}:latest"
          echo "   - ${IMAGE_NAME}:${IMAGE_TAG}"
          echo "   - ${IMAGE_NAME}:${MODEL_TAG}"
          
          # Save to env for next step
          echo "IMAGE_NAME=${IMAGE_NAME}" >> $GITHUB_ENV
          echo "IMAGE_TAG=${IMAGE_TAG}" >> $GITHUB_ENV
      
      - name: Push Docker image to Docker Hub
        env:
          IMAGE_NAME: ${{ env.IMAGE_NAME }}
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          MODEL_TAG: ${{ steps.best-model.outputs.model_tag }}
        run: |
          echo "Pushing Docker images to Docker Hub..."
          
          # Push all tags
          docker push ${IMAGE_NAME}:latest
          docker push ${IMAGE_NAME}:${IMAGE_TAG}
          docker push ${IMAGE_NAME}:${MODEL_TAG}
          
          echo "‚úÖ Docker images pushed successfully!"
          echo "   - ${IMAGE_NAME}:latest"
          echo "   - ${IMAGE_NAME}:${IMAGE_TAG}"
          echo "   - ${IMAGE_NAME}:${MODEL_TAG}"
      
      - name: Create Docker summary
        env:
          IMAGE_NAME: ${{ env.IMAGE_NAME }}
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
          MODEL_NAME: ${{ steps.best-model.outputs.model_name }}
          ACCURACY: ${{ steps.best-model.outputs.accuracy }}
        run: |
          echo "## üê≥ Docker Image Published" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Model:** ${MODEL_NAME}" >> $GITHUB_STEP_SUMMARY
          echo "**Accuracy:** ${ACCURACY}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Docker Images:" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "docker pull ${IMAGE_NAME}:latest" >> $GITHUB_STEP_SUMMARY
          echo "docker pull ${IMAGE_NAME}:${IMAGE_TAG}" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Run Container:" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo "docker run -p 8080:8080 ${IMAGE_NAME}:latest" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

  notify:
    name: Notification
    needs: [train, evaluate, docker]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Training Success Notification
        if: needs.train.result == 'success' && needs.evaluate.result == 'success'
        run: |
          echo "‚úÖ All models trained successfully!"
          echo "üìä Evaluation completed!"
      
      - name: Docker Success Notification
        if: needs.docker.result == 'success'
        run: |
          echo "üê≥ Docker image pushed to Docker Hub!"
      
      - name: Docker Skipped Notification
        if: needs.docker.result == 'skipped'
        run: |
          echo "‚ö†Ô∏è Docker build was skipped"
          echo ""
          echo "Possible reasons:"
          echo "  1. DOCKER_USERNAME secret not set"
          echo "  2. DOCKER_PASSWORD secret not set"
          echo ""
          echo "To enable Docker deployment:"
          echo "  1. Go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
          echo "  2. Add secret: DOCKER_USERNAME (your Docker Hub username)"
          echo "  3. Add secret: DOCKER_PASSWORD (your Docker Hub access token)"
          echo ""
          echo "üìñ See DOCKER_SETUP.md for detailed instructions"
      
      - name: Training Failure Notification
        if: needs.train.result == 'failure' || needs.evaluate.result == 'failure'
        run: |
          echo "‚ùå Training or evaluation failed!"
          echo "Please check the logs for details."
          exit 1
      
      - name: Docker Failure Notification
        if: needs.docker.result == 'failure'
        run: |
          echo "‚ö†Ô∏è Docker build/push failed!"
          echo "Training was successful but Docker image not created."
          echo ""
          echo "Common issues:"
          echo "  1. Check DOCKER_USERNAME and DOCKER_PASSWORD secrets"
          echo "  2. Verify Docker Hub access token has correct permissions"
          echo "  3. Check Docker Hub storage quota"
          echo "  4. Review workflow logs for specific error"
